{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optimum","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install auto-gptq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gptqmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall scipy -y\n!pip install scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport os\n# تعطيل Flash Attention لتوفير الذاكرة\nos.environ[\"FLASH_ATTENTION\"] = \"0\"\n\nos.environ[\"TORCH_COMPILE\"] = \"0\"\n\n# مسار تفريغ أجزاء النموذج إلى الهارد\noffload_dir = \"./offload\"\n\n# معرف النموذج\nmodel_id = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنيزر\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# تحديد إدارة الذاكرة على GPU و RAM \n# Removed \"disk\" from max_memory\nmax_memory = {\n    0: \"14GiB\",  # تخصيص 13 جيجابايت للـ GPU\n    \"cpu\": \"12GiB\",  # استخدام 32 جيجابايت من RAM\n}\n\n# تحميل النموذج مع إدارة الذاكرة\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    offload_folder=offload_dir,  # using offloading to manage memory\n    offload_state_dict=True, # enabling offloading for the model's state dictionary\n    max_memory=max_memory,\n    use_safetensors=True  # to solve potential issues with offloading and loading\n)\n\n# إعداد pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# تجربة التوليد\nprompt = \"مرحبا، كيف يمكنني مساعدتك اليوم؟\"\noutput = pipe(prompt, max_new_tokens=1)\n\n# طباعة النتيجة\nprint(output[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"شغال ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y numpy scipy\n!pip install numpy==1.26.0 scipy==1.11.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPTQModel.from_quantized(quant_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنايزر\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج وتوزيعه تلقائيًا على كرتي الشاشة\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# تجربة التوليد\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=20)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنايزر\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج وتوزيعه تلقائيًا على كرتين GPU\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# اختبار التوليد\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda:0\")  # إرسال الإدخال إلى كرت الشاشة الأول\noutputs = model.generate(**inputs, max_new_tokens=50)  # تحديد عدد التوكنات الناتجة\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج على كرت شاشة واحد أولًا\nmodel = GPTQModel.from_quantized(quant_dir, torch_dtype=torch.float16)\n\n# توزيع النموذج على كرتي شاشة\nmodel = torch.nn.DataParallel(model)  \nmodel.to(\"cuda\")  # تأكد من تشغيله على الـ GPU\n\n# تمرير الإدخال إلى النموذج\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.module.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!!nvidia-smi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج على كرت شاشة واحد أولًا\nmodel = GPTQModel.from_quantized(quant_dir, torch_dtype=torch.float16)\n\n# توزيع النموذج على كرتي شاشة\nmodel = torch.nn.DataParallel(model)  \nmodel.to(\"cuda\")  # تأكد من تشغيله على الـ GPU\n\n# تمرير الإدخال إلى النموذج\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.module.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda:0\")  # استخدام GPU 0\ndevice = torch.device(\"cuda:1\")  # استخدام GPU 1\n\nprint(\"استخدام كرت:\", device_0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج مع توزيع تلقائي على كرتي الشاشة\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# تمرير الإدخال إلى الجهاز الصحيح\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda:0\", \"cuda:1\")\noutputs = model.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n# Load the model pipeline\npipe = pipeline(\"text-generation\", model=\"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\", trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16)\n\n# Use a properly formatted string as input, with do_sample\nresponse = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True) \n\n# Print response\nprint(response[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T00:30:56.671243Z","iopub.execute_input":"2025-02-25T00:30:56.671582Z","iopub.status.idle":"2025-02-25T00:34:01.661023Z","shell.execute_reply.started":"2025-02-25T00:30:56.671559Z","shell.execute_reply":"2025-02-25T00:34:01.660072Z"}},"outputs":[{"name":"stderr","text":"Detected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"name":"stdout","text":"ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\nENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for compatibililty.\n","output_type":"stream"},{"name":"stderr","text":"INFO - Auto pick kernel based on compatibility: <class 'gptqmodel.nn_modules.qlinear.dynamic_cuda.DynamicCudaQuantLinear'>\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nDetected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6d2c1794a14f4fabfcf03e44993063"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Who are you? You know who I am? Are you sure?\"\n\"Of course I am!\" I said, feeling a bit\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}