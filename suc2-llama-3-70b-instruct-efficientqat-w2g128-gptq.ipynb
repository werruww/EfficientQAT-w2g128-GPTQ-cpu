{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optimum","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install auto-gptq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gptqmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall scipy -y\n!pip install scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport os\n# تعطيل Flash Attention لتوفير الذاكرة\nos.environ[\"FLASH_ATTENTION\"] = \"0\"\n\nos.environ[\"TORCH_COMPILE\"] = \"0\"\n\n# مسار تفريغ أجزاء النموذج إلى الهارد\noffload_dir = \"./offload\"\n\n# معرف النموذج\nmodel_id = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنيزر\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# تحديد إدارة الذاكرة على GPU و RAM \n# Removed \"disk\" from max_memory\nmax_memory = {\n    0: \"14GiB\",  # تخصيص 13 جيجابايت للـ GPU\n    \"cpu\": \"12GiB\",  # استخدام 32 جيجابايت من RAM\n}\n\n# تحميل النموذج مع إدارة الذاكرة\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    offload_folder=offload_dir,  # using offloading to manage memory\n    offload_state_dict=True, # enabling offloading for the model's state dictionary\n    max_memory=max_memory,\n    use_safetensors=True  # to solve potential issues with offloading and loading\n)\n\n# إعداد pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# تجربة التوليد\nprompt = \"مرحبا، كيف يمكنني مساعدتك اليوم؟\"\noutput = pipe(prompt, max_new_tokens=1)\n\n# طباعة النتيجة\nprint(output[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"شغال ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y numpy scipy\n!pip install numpy==1.26.0 scipy==1.11.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPTQModel.from_quantized(quant_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنايزر\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج وتوزيعه تلقائيًا على كرتي الشاشة\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# تجربة التوليد\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=20)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir)\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# تحميل التوكنايزر\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج وتوزيعه تلقائيًا على كرتين GPU\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# اختبار التوليد\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda:0\")  # إرسال الإدخال إلى كرت الشاشة الأول\noutputs = model.generate(**inputs, max_new_tokens=50)  # تحديد عدد التوكنات الناتجة\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج على كرت شاشة واحد أولًا\nmodel = GPTQModel.from_quantized(quant_dir, torch_dtype=torch.float16)\n\n# توزيع النموذج على كرتي شاشة\nmodel = torch.nn.DataParallel(model)  \nmodel.to(\"cuda\")  # تأكد من تشغيله على الـ GPU\n\n# تمرير الإدخال إلى النموذج\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.module.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!!nvidia-smi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج على كرت شاشة واحد أولًا\nmodel = GPTQModel.from_quantized(quant_dir, torch_dtype=torch.float16)\n\n# توزيع النموذج على كرتي شاشة\nmodel = torch.nn.DataParallel(model)  \nmodel.to(\"cuda\")  # تأكد من تشغيله على الـ GPU\n\n# تمرير الإدخال إلى النموذج\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.module.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda:0\")  # استخدام GPU 0\ndevice = torch.device(\"cuda:1\")  # استخدام GPU 1\n\nprint(\"استخدام كرت:\", device_0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج مع توزيع تلقائي على كرتي الشاشة\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\", torch_dtype=torch.float16)\n\n# تمرير الإدخال إلى الجهاز الصحيح\ninputs = tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(\"cuda:0\", \"cuda:1\")\noutputs = model.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n# Load the model pipeline\npipe = pipeline(\"text-generation\", model=\"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\", trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16)\n\n# Use a properly formatted string as input, with do_sample\nresponse = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True) \n\n# Print response\nprint(response[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# Specify device_map for two GPUs\ndevice_map = {\n    0: 0,  # Layer 0 to GPU 0\n    1: 1,  # Layer 1 to GPU 1\n    'transformer.word_embeddings': 0,  # Word embeddings to GPU 0\n    'transformer.final_layernorm': 1,  # Final layer norm to GPU 1\n    'lm_head': 1  # Language model head to GPU 1\n}\n\n# Load quantized model, distributing layers across GPUs\nmodel = GPTQModel.from_quantized(quant_dir, device_map=device_map) \n\n# Inference\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# Modified device_map for potential better distribution\ndevice_map = {\n    \"transformer.layers.0\": 0,\n    \"transformer.layers.1\": 1,\n    \"transformer.layers.2\": 0,\n    \"transformer.layers.3\": 1,\n    # ... (Continue mapping layers to GPUs 0 and 1 alternately)\n    \"transformer.word_embeddings\": 0,\n    \"transformer.final_layernorm\": 1,\n    \"lm_head\": 1\n}\n\n# Load quantized model, distributing layers across GPUs\nmodel = GPTQModel.from_quantized(quant_dir, device_map=device_map)\n\n# Inference\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n# or local path\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n\n# load quantized model to the first GPU\nmodel = GPTQModel.from_quantized(quant_dir, device_map=\"auto\")\n\n# inference with model.generate\nprint(tokenizer.decode(model.generate(**tokenizer(\"Model quantization is\", return_tensors=\"pt\").to(model.device))[0]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n# quant_dir = \"ChenMnZ/Llama-2-7b-EfficientQAT-w2g128-BitBLAS\"\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# Create a specific device map to distribute across both GPUs\n# For a large model like Llama-3-70b, this is important\ndevice_map = {\n    # You can customize this based on model architecture\n    # This is a simplified example - adjust layer distribution as needed\n    \"model.embed_tokens\": 0,\n    \"model.layers.0\": 0,\n    \"model.layers.1\": 0,\n    # ... distribute more layers between GPUs 0 and 1\n    \"model.layers.15\": 1,\n    \"model.layers.16\": 1,\n    # ... and so on\n    \"model.norm\": 1,\n    \"lm_head\": 1\n}\n\n# Alternatively, you can use \"balanced\" strategy that automatically balances across GPUs\n# device_map = \"balanced\"\n\n# Load model with custom device map\nmodel = GPTQModel.from_quantized(quant_dir, device_map=device_map)\n\n# Run inference\ninput_text = \"Model quantization is\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**input_ids)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig\nfrom gptqmodel import GPTQModel\nimport torch\nfrom accelerate import Accelerator, dispatch_model\n\n# تهيئة المسرع\naccelerator = Accelerator()\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# تحميل النموذج أولاً بدون تعيين device_map\nmodel = GPTQModel.from_quantized(quant_dir, device_map=None)\n\n# استخدام المسرع لتوزيع النموذج عبر جميع أجهزة GPU المتاحة\nmodel = accelerator.prepare(model)\n\n# طباعة معلومات عن استخدام GPU\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB / {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\ninputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\nwith torch.no_grad():\n    output = model.generate(**inputs)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, LlamaForCausalLM\nimport torch\n\n# استخدام الواجهة البرمجية للتحويلات مباشرة مع دعم GPTQ\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# التحقق من توفر أجهزة GPU\ngpu_count = torch.cuda.device_count()\nprint(f\"عدد أجهزة GPU المتاحة: {gpu_count}\")\n\n# أولاً، التحقق من مساحة الذاكرة المتاحة على كل GPU\nfor i in range(gpu_count):\n    free_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n    print(f\"GPU {i}: {free_mem:.2f} GB ذاكرة إجمالية\")\n\n# تعيين max_memory لكل جهاز GPU\nmax_memory = {i: f\"{int(torch.cuda.get_device_properties(i).total_memory / 1024**3 * 0.9)}GiB\" for i in range(gpu_count)}\nprint(f\"توزيع الذاكرة: {max_memory}\")\n\n# استخدام LlamaForCausalLM بدلاً من GPTQModel (التي تدعم GPTQ تلقائياً في transformers الحديثة)\nmodel = LlamaForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=\"auto\",  # سيقوم بتقسيم النموذج تلقائياً بناءً على الذاكرة المتاحة\n    max_memory=max_memory,\n    torch_dtype=torch.float16,  # استخدام float16 لتوفير الذاكرة\n    load_in_4bit=True  # تأكيد تحميل النموذج المُكَمّ\n)\n\n# طباعة خريطة الأجهزة النهائية لمعرفة كيفية توزيع النموذج\nprint(\"خريطة الأجهزة النهائية:\")\nfor name, device in model.hf_device_map.items():\n    print(f\"{name}: {device}\")\n\n# طباعة استخدام الذاكرة بعد تحميل النموذج\nfor i in range(gpu_count):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n# لا تحتاج إلى نقل المدخلات يدوياً لأن النموذج سيتعامل مع ذلك\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# إنشاء خريطة أجهزة مخصصة (توزيع يدوي أكثر)\nnum_layers = 32  # قد تحتاج إلى ضبط هذا بناءً على معمارية النموذج\n\ndevice_map = {\n    \"model.embed_tokens\": 0,\n    \"model.norm\": 1,\n    \"lm_head\": 1,\n}\n\n# توزيع الطبقات على جهازي GPU\nfor i in range(num_layers):\n    if i < num_layers // 2:\n        device_map[f\"model.layers.{i}\"] = 0\n    else:\n        device_map[f\"model.layers.{i}\"] = 1\n\nprint(\"خريطة الأجهزة المخصصة:\")\nprint(device_map)\n\n# تحميل النموذج باستخدام AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=device_map,\n    torch_dtype=torch.float16  # استخدام float16 للتوافق\n)\n\n# التحقق من استخدام الذاكرة على كل GPU\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# أولاً تأكد من تثبيت auto-gptq\n# pip install auto-gptq\n\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# التحقق من عدد أجهزة GPU المتاحة\ngpu_count = torch.cuda.device_count()\nprint(f\"عدد أجهزة GPU المتاحة: {gpu_count}\")\n\n# تعيين max_memory لكل جهاز GPU\nmax_memory = {i: f\"{int(torch.cuda.get_device_properties(i).total_memory / 1024**3 * 0.9)}GiB\" for i in range(gpu_count)}\nprint(f\"توزيع الذاكرة: {max_memory}\")\n\n# تحميل نموذج GPTQ باستخدام AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n    quant_dir,\n    use_triton=False,  # قد تحتاج إلى ضبط هذا حسب بيئتك\n    device_map=\"auto\",  # سيقوم بتقسيم النموذج تلقائيًا\n    max_memory=max_memory,\n    quantize_config=None  # سيتم اكتشافه تلقائيًا من الملفات\n)\n\n# طباعة معلومات عن استخدام الذاكرة\nfor i in range(gpu_count):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# في حالة وجود خريطة أجهزة، طباعتها\nif hasattr(model, \"hf_device_map\"):\n    print(\"خريطة الأجهزة النهائية:\")\n    for name, device in model.hf_device_map.items():\n        print(f\"{name}: {device}\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"شغال الكرتين ولكن اخطاء","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\n\n# الحصول على معلومات عن معمارية النموذج أولاً\ntry:\n    config = AutoConfig.from_pretrained(quant_dir)\n    num_layers = config.num_hidden_layers\n    print(f\"عدد الطبقات في النموذج: {num_layers}\")\nexcept Exception as e:\n    print(f\"خطأ في قراءة التكوين: {e}\")\n    # اذا فشل قراءة عدد الطبقات، استخدم قيمة محافظة\n    if \"70b\" in quant_dir:\n        num_layers = 80  # قيمة تقديرية للنموذج 70B\n    else:\n        num_layers = 32  # قيمة افتراضية\n\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# إنشاء خريطة أجهزة مخصصة\ndevice_map = {\n    \"model.embed_tokens\": 0,\n    \"model.norm\": 1,\n    \"lm_head\": 1,\n}\n\n# التأكد من أن عدد الطبقات صحيح\nmiddle_layer = num_layers // 2\nprint(f\"سيتم وضع الطبقات 0-{middle_layer-1} على GPU 0\")\nprint(f\"سيتم وضع الطبقات {middle_layer}-{num_layers-1} على GPU 1\")\n\n# توزيع الطبقات على جهازي GPU\nfor i in range(num_layers):\n    if i < middle_layer:\n        device_map[f\"model.layers.{i}\"] = 0\n    else:\n        device_map[f\"model.layers.{i}\"] = 1\n\nprint(\"خريطة الأجهزة:\")\nprint(device_map)\n\n# تحميل النموذج باستخدام خريطة الأجهزة المخصصة\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        quant_dir,\n        device_map=device_map,\n        torch_dtype=torch.float16  # استخدام float16 للتوافق\n    )\n    print(\"تم تحميل النموذج بنجاح!\")\nexcept Exception as e:\n    print(f\"خطأ في تحميل النموذج: {e}\")\n    # محاولة بديلة باستخدام device_map=\"auto\"\n    print(\"محاولة استخدام device_map تلقائية...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        quant_dir,\n        device_map=\"auto\",\n        max_memory={0: \"12GiB\", 1: \"12GiB\"},  # ضبط حسب الذاكرة المتاحة لديك\n        torch_dtype=torch.float16\n    )\n\n# التحقق من استخدام الذاكرة على كل GPU\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, LlamaForCausalLM\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# استخدام device_map=\"balanced\" لتوزيع متوازن\nmodel = LlamaForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=\"auto\",  # يجب أن يوزع بالتساوي بين الأجهزة المتاحة\n    load_in_8bit=False,     # لا تستخدم تكميم 8-bit\n    load_in_4bit=False      # لا تستخدم تكميم 4-bit\n)\n\n# طباعة معلومات عن استخدام الذاكرة\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T01:03:31.814946Z","iopub.execute_input":"2025-02-25T01:03:31.815487Z","iopub.status.idle":"2025-02-25T01:07:17.952862Z","shell.execute_reply.started":"2025-02-25T01:03:31.815435Z","shell.execute_reply":"2025-02-25T01:07:17.951702Z"}},"outputs":[{"name":"stderr","text":"Detected gptqmodel and auto-gptq, will use gptqmodel\nINFO - Auto pick kernel based on compatibility: <class 'gptqmodel.nn_modules.qlinear.dynamic_cuda.DynamicCudaQuantLinear'>\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nDetected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8c1ab463a44ede9d75927116ab3c6f"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"GPU 0: 8.79 GB مستخدمة\nGPU 1: 12.21 GB مستخدمة\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model quantization is a good starting point for understanding the concept of a \"good\" or \"bad\" person. It is important to recognize that people are complex and multifaceted, and that their actions and beliefs can be influenced by a wide range\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}