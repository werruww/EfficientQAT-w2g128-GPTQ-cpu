{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woYgT-aqPa7c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum"
      ],
      "metadata": {
        "id": "hE91SEmRPcCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq"
      ],
      "metadata": {
        "id": "XH2b-DlgPfHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel"
      ],
      "metadata": {
        "id": "M--35_VkPgSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChenMnZ/Llama-2-70b-EfficientQAT-w2g128-GPTQ"
      ],
      "metadata": {
        "id": "utv9QR3eRrco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "id": "WNd16bT0R55W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import os\n",
        "# تعطيل Flash Attention لتوفير الذاكرة\n",
        "os.environ[\"FLASH_ATTENTION\"] = \"0\"\n",
        "\n",
        "os.environ[\"TORCH_COMPILE\"] = \"0\"\n",
        "\n",
        "# مسار تفريغ أجزاء النموذج إلى الهارد\n",
        "offload_dir = \"./offload\"\n",
        "\n",
        "# معرف النموذج\n",
        "model_id = \"ChenMnZ/Llama-2-70b-EfficientQAT-w2g128-GPTQ\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# تحديد إدارة الذاكرة على GPU و RAM\n",
        "# Removed \"disk\" from max_memory\n",
        "max_memory = {\n",
        "    0: \"12GiB\",  # تخصيص 13 جيجابايت للـ GPU\n",
        "    \"cpu\": \"12GiB\",  # استخدام 32 جيجابايت من RAM\n",
        "}\n",
        "\n",
        "# تحميل النموذج مع إدارة الذاكرة\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    offload_folder=offload_dir,  # using offloading to manage memory\n",
        "    offload_state_dict=True, # enabling offloading for the model's state dictionary\n",
        "    max_memory=max_memory,\n",
        "    use_safetensors=True  # to solve potential issues with offloading and loading\n",
        ")\n",
        "\n",
        "# إعداد pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# تجربة التوليد\n",
        "prompt = \"مرحبا، كيف يمكنني مساعدتك اليوم؟\"\n",
        "output = pipe(prompt, max_new_tokens=1)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "SN_hqRDUPcFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip uninstall scipy -y\n",
        "!pip install scipy"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AAYWrUSpSAsX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}